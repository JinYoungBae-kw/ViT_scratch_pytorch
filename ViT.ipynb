{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1QI9ylaH8jN"
      },
      "source": [
        "## ViT 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention"
      ],
      "metadata": {
        "id": "8rIjWEmb2LQR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oHSQAYNOsC9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb1b90b-8a3f-4187-b785-2549e0b11544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "from einops import rearrange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, patches_dim, n_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.root_dk = torch.sqrt(torch.tensor(patches_dim / n_heads, dtype=torch.float32))\n",
        "\n",
        "        self.q_Linear = nn.Linear(patches_dim, patches_dim)\n",
        "        self.k_Linear = nn.Linear(patches_dim, patches_dim)\n",
        "        self.v_Linear = nn.Linear(patches_dim, patches_dim)\n",
        "        self.last_Linear = nn.Linear(patches_dim, patches_dim)\n",
        "        self.linear_layers = [self.q_Linear, self.k_Linear, self.v_Linear, self.last_Linear]\n",
        "\n",
        "        for layer in self.linear_layers:\n",
        "            init.xavier_uniform_(layer.weight)\n",
        "            if layer.bias is not None:\n",
        "                init.constant_(layer.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q, K, V = [linear(x) for linear in self.linear_layers[:3]]\n",
        "        Q, K, V = [rearrange(tensor, 'b n (h d) -> b h n d', h=self.n_heads) for tensor in [Q, K, V]]\n",
        "\n",
        "        qkt_dk = torch.matmul(Q, K.transpose(-2,-1)) / self.root_dk\n",
        "        s_qkt_dk = torch.softmax(qkt_dk, dim=-1)\n",
        "        attention_result = torch.matmul(s_qkt_dk, V)\n",
        "\n",
        "        concat_out = rearrange(attention_result, 'b h n d -> b n (h d)')\n",
        "        result = self.last_Linear(concat_out)\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP"
      ],
      "metadata": {
        "id": "NrYL8vA62oJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP (Linear -> GELU -> Dropout -> Linear)\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, patches_dim, n_hidden_layer, drop_p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(patches_dim, n_hidden_layer),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(n_hidden_layer, patches_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.mlp(x)\n",
        "        return result"
      ],
      "metadata": {
        "id": "bZDN3-2C2TSE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "POjS3gVz6yyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder 블럭\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, patches_dim, n_hidden_layer, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.first_norm = nn.LayerNorm(patches_dim, eps=1e-6)\n",
        "        self.self_attention = MultiHeadAttention(patches_dim, n_heads)\n",
        "        self.second_norm = nn.LayerNorm(patches_dim, eps=1e-6)\n",
        "        self.mlp = MLP(patches_dim, n_hidden_layer, drop_p)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm_out = self.first_norm(x)\n",
        "        attention_result = self.self_attention(norm_out)\n",
        "        attention_result = self.dropout(attention_result)\n",
        "        mha_result_with_skip = x + attention_result\n",
        "\n",
        "        norm_out = self.second_norm(mha_result_with_skip)\n",
        "        mlp_out = self.mlp(norm_out)\n",
        "        mlp_out = self.dropout(mlp_out)\n",
        "        result_with_skip = mha_result_with_skip + mlp_out\n",
        "\n",
        "        return result_with_skip\n",
        "\n",
        "# Encoder 실제 작동\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_patches_with_cls, n_layers, patches_dim, n_hidden_layer, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.position_embedding = nn.Parameter(0.01 * torch.randn(n_patches_with_cls, patches_dim))\n",
        "        self.encoder_blocks = nn.ModuleList([EncoderBlock(patches_dim, n_hidden_layer, n_heads, drop_p) for _ in range(n_layers)])\n",
        "        self.norm_for_cls = nn.LayerNorm(patches_dim, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ready_for_encoder = x + self.position_embedding\n",
        "        ready_for_encoder = self.dropout(ready_for_encoder)\n",
        "\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            encoder_result = encoder_block(ready_for_encoder)\n",
        "\n",
        "        CLS = encoder_result[:,0,:]\n",
        "        ready_for_head = self.norm_for_cls(CLS)\n",
        "\n",
        "        return ready_for_head"
      ],
      "metadata": {
        "id": "5_2J4QRf6exJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT Model"
      ],
      "metadata": {
        "id": "5tm6GPnX7GI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, n_layers, patches_dim, n_hidden_layer, n_heads, n_MlpHead_hidden_layer, drop_p, n_classes, fine_tuning):\n",
        "        super().__init__()\n",
        "\n",
        "        seq_length_with_cls = (image_size // patch_size) ** 2 + 1\n",
        "        self.cls = nn.Parameter(torch.randn(patches_dim) * 0.02)\n",
        "        self.encoder = Encoder(seq_length_with_cls, n_layers, patches_dim, n_hidden_layer, n_heads, drop_p)\n",
        "        self.patch_embedding = nn.Conv2d(3, patches_dim, patch_size, stride=patch_size)\n",
        "\n",
        "        # conv\n",
        "        fan_in = self.patch_embedding.in_channels * self.patch_embedding.kernel_size[0] * self.patch_embedding.kernel_size[1]\n",
        "        init.trunc_normal_(self.patch_embedding.weight, std=math.sqrt(1 / fan_in))\n",
        "        if self.patch_embedding.bias is not None:\n",
        "            init.zeros_(self.patch_embedding.bias)\n",
        "\n",
        "        # head\n",
        "        if fine_tuning:\n",
        "            self.head = nn.Linear(patches_dim, n_classes)\n",
        "            init.zeros_(self.head.weight)\n",
        "            init.zeros_(self.head.bias)\n",
        "        else:\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(patches_dim, n_MlpHead_hidden_layer),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_MlpHead_hidden_layer, n_classes))\n",
        "            fan_in = self.head[0].in_features\n",
        "            init.trunc_normal_(self.head[0].weight, std=math.sqrt(1 / fan_in))\n",
        "            init.zeros_(self.head[0].bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.patch_embedding(x)\n",
        "        ready_patches = rearrange(patches, 'b d ph pw -> b (ph pw) d')\n",
        "\n",
        "        batch_cls = self.cls.expand(ready_patches.shape[0], 1, -1)\n",
        "        all_patches_with_cls = torch.cat([batch_cls, ready_patches], dim=1)\n",
        "\n",
        "        encoder_out = self.encoder(all_patches_with_cls)\n",
        "\n",
        "        model_result = self.head(encoder_out)\n",
        "\n",
        "        return model_result"
      ],
      "metadata": {
        "id": "FGp5ykZijgOz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWkfP-xEToVX"
      },
      "source": [
        "## 모델 생성 예시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s8iv4xnBIqVG"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = ViT(\n",
        "    image_size = 224,\n",
        "    patch_size = 16,\n",
        "    n_layers = 12,\n",
        "    patches_dim = 768,\n",
        "    n_hidden_layer = 3072,\n",
        "    n_heads = 12,\n",
        "    n_MlpHead_hidden_layer = 512,\n",
        "    drop_p = 0.1,\n",
        "    n_classes = 1000,\n",
        "    fine_tuning = False).to(DEVICE)\n",
        "\n",
        "summary(model, input_size=(2, 3, 224, 224), device=DEVICE)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}